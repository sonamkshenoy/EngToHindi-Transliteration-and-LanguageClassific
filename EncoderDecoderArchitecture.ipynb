{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EncoderDecoderArchitecture.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPjotxofntpcNu28SmKTfX0"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ewFbZLpa6Dm",
        "colab_type": "text"
      },
      "source": [
        "# Outline\n",
        "\n",
        "1. Data set and task\n",
        "2. Data processing XML files\n",
        "3. Why we need encoder decoder architecture\n",
        "4. Basic GRU based encoder decoder\n",
        "5. Adding attention\n",
        "6. Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLYVFJufavdk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "# Instantiates the device to be used as GPU/CPU based on availability\n",
        "device_gpu = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-YpfrGmEbobB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "33cca475-1e5e-47db-e8da-74596c7ddb4e"
      },
      "source": [
        "# Visualisation tools\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from IPython.display import clear_output\n",
        "import random"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NlBBjyU8b324",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1YeJz5dcitT",
        "colab_type": "text"
      },
      "source": [
        "# Prepare Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "476ASkj-clYp",
        "colab_type": "text"
      },
      "source": [
        "### Alphabets setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ew944iDyeaxt",
        "colab_type": "text"
      },
      "source": [
        "As usual, convert first to numerical form so that model can process."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BoKmPaCce-7r",
        "colab_type": "text"
      },
      "source": [
        "### English"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9_wc9lAcjp7",
        "colab_type": "code",
        "outputId": "382c7448-c2f8-46bd-c1f9-e3cb1a583040",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 56
        }
      },
      "source": [
        "eng_alphabets = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'\n",
        "pad_char = '-PAD-'\n",
        "\n",
        "eng_alpha2index = {pad_char:0}\n",
        "for index, alpha in enumerate(eng_alphabets):\n",
        "  eng_alpha2index[alpha] = index+1\n",
        "\n",
        "# Same as:\n",
        "# for index, alpha in enumerate(eng_alphabets, 1):\n",
        "#   eng_alpha2index[alpha] = index\n",
        "\n",
        "print(eng_alpha2index)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'-PAD-': 0, 'A': 1, 'B': 2, 'C': 3, 'D': 4, 'E': 5, 'F': 6, 'G': 7, 'H': 8, 'I': 9, 'J': 10, 'K': 11, 'L': 12, 'M': 13, 'N': 14, 'O': 15, 'P': 16, 'Q': 17, 'R': 18, 'S': 19, 'T': 20, 'U': 21, 'V': 22, 'W': 23, 'X': 24, 'Y': 25, 'Z': 26}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4BcT4Yzez35",
        "colab_type": "text"
      },
      "source": [
        "### Hindi"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQ_ZADU-dHm0",
        "colab_type": "code",
        "outputId": "f342ff7a-038a-4698-e7f0-c3c33d1cbecc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 56
        }
      },
      "source": [
        "# Hindi Unicode Hex Range is 2304:2432\n",
        "\n",
        "hindi_alphabets = [chr(alpha) for alpha in range(2304, 2432)]\n",
        "hindi_alphabet_size = len(hindi_alphabets)\n",
        "\n",
        "hindi_alpha2index = {pad_char : 0}\n",
        "for index, alpha in enumerate(hindi_alphabets):\n",
        "  hindi_alpha2index[alpha] = index + 1\n",
        "\n",
        "print(hindi_alpha2index)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'-PAD-': 0, 'ऀ': 1, 'ँ': 2, 'ं': 3, 'ः': 4, 'ऄ': 5, 'अ': 6, 'आ': 7, 'इ': 8, 'ई': 9, 'उ': 10, 'ऊ': 11, 'ऋ': 12, 'ऌ': 13, 'ऍ': 14, 'ऎ': 15, 'ए': 16, 'ऐ': 17, 'ऑ': 18, 'ऒ': 19, 'ओ': 20, 'औ': 21, 'क': 22, 'ख': 23, 'ग': 24, 'घ': 25, 'ङ': 26, 'च': 27, 'छ': 28, 'ज': 29, 'झ': 30, 'ञ': 31, 'ट': 32, 'ठ': 33, 'ड': 34, 'ढ': 35, 'ण': 36, 'त': 37, 'थ': 38, 'द': 39, 'ध': 40, 'न': 41, 'ऩ': 42, 'प': 43, 'फ': 44, 'ब': 45, 'भ': 46, 'म': 47, 'य': 48, 'र': 49, 'ऱ': 50, 'ल': 51, 'ळ': 52, 'ऴ': 53, 'व': 54, 'श': 55, 'ष': 56, 'स': 57, 'ह': 58, 'ऺ': 59, 'ऻ': 60, '़': 61, 'ऽ': 62, 'ा': 63, 'ि': 64, 'ी': 65, 'ु': 66, 'ू': 67, 'ृ': 68, 'ॄ': 69, 'ॅ': 70, 'ॆ': 71, 'े': 72, 'ै': 73, 'ॉ': 74, 'ॊ': 75, 'ो': 76, 'ौ': 77, '्': 78, 'ॎ': 79, 'ॏ': 80, 'ॐ': 81, '॑': 82, '॒': 83, '॓': 84, '॔': 85, 'ॕ': 86, 'ॖ': 87, 'ॗ': 88, 'क़': 89, 'ख़': 90, 'ग़': 91, 'ज़': 92, 'ड़': 93, 'ढ़': 94, 'फ़': 95, 'य़': 96, 'ॠ': 97, 'ॡ': 98, 'ॢ': 99, 'ॣ': 100, '।': 101, '॥': 102, '०': 103, '१': 104, '२': 105, '३': 106, '४': 107, '५': 108, '६': 109, '७': 110, '८': 111, '९': 112, '॰': 113, 'ॱ': 114, 'ॲ': 115, 'ॳ': 116, 'ॴ': 117, 'ॵ': 118, 'ॶ': 119, 'ॷ': 120, 'ॸ': 121, 'ॹ': 122, 'ॺ': 123, 'ॻ': 124, 'ॼ': 125, 'ॽ': 126, 'ॾ': 127, 'ॿ': 128}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QbBoHqqgWEy",
        "colab_type": "text"
      },
      "source": [
        "Note: de in devanagiri in Hindi, though looks like one character (in Hindi), is actually 2 unicode characters : da and e (in hindi)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ssw7bkbcgElo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zqgjb2HWgIR3",
        "colab_type": "text"
      },
      "source": [
        "# Data Pre-processing helper functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "001Zhv85gLgE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re # regular expressions\n",
        "non_eng_letters_regex = re.compile('[^a-zA-Z ]')\n",
        "\n",
        "# Remove all English non-letters (alphabets and space)\n",
        "def cleanEnglishVocab(line):\n",
        "  line = line.replace('-',' ').replace(',',' ').upper() # '-' and ',' act as space\n",
        "  line = non_eng_letters_regex.sub('', line) # substitute all chars of non_eng_letters_regex present in line by nothing ('')\n",
        "  return line.split()\n",
        "\n",
        "# Remove all Hindi non-letters\n",
        "def cleanHindiVocab(line):\n",
        "  # print(line)\n",
        "  line = line.replace('-',' ').replace(',',' ')\n",
        "  cleaned_line = ''\n",
        "  for char in line:\n",
        "    if char in hindi_alpha2index or char == ' ':\n",
        "      cleaned_line += char\n",
        "  return cleaned_line.split() # mistake : gave it one indent more so always returned length 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpMaU2DUiNS9",
        "colab_type": "text"
      },
      "source": [
        "# Dataset Loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8n3fovAdiI3S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import Dataset\n",
        "import xml.etree.ElementTree as ET # available in Python"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8EBkFmfbyUlU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tree = ET.parse('NEWS2012-Ref-EnHi-1000.xml')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNVUETo1z3kT",
        "colab_type": "code",
        "outputId": "26025c8a-82f1-40b5-d085-c232284e446e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "root = tree.getroot()\n",
        "print(root)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<Element 'TransliterationCorpus' at 0x7fec88426d68>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVyqSEw_z4TG",
        "colab_type": "code",
        "outputId": "8b5675f8-5c98-4342-d213-e17d0dab4e0a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "root.tag"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'TransliterationCorpus'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jfnf6WMa0gz5",
        "colab_type": "text"
      },
      "source": [
        "The root tag is named TransliterationCorpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCeLQPlb0a1j",
        "colab_type": "code",
        "outputId": "dccb6a30-0950-4380-e30d-7c8fa04bffb9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        }
      },
      "source": [
        "i = 0\n",
        "for child in root:\n",
        "    print(child.tag, child.attrib)\n",
        "    if i > 5:\n",
        "      break\n",
        "    i += 1"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Name {'ID': '1'}\n",
            "Name {'ID': '2'}\n",
            "Name {'ID': '3'}\n",
            "Name {'ID': '4'}\n",
            "Name {'ID': '5'}\n",
            "Name {'ID': '6'}\n",
            "Name {'ID': '7'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0ieeJwn0_-Q",
        "colab_type": "text"
      },
      "source": [
        "The next sub level tags are \"Name\" tags with \"ID\" attribute.  \n",
        "Similar to how a HTML tag div, has an attribute \"class\" or \"id\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0j0GsW81Tpo",
        "colab_type": "code",
        "outputId": "e3f41df5-9764-4ab5-bb2a-18ba08c0401a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        }
      },
      "source": [
        "i = 0\n",
        "for elem in root.iter():\n",
        "    print(elem.tag)\n",
        "    if i > 11:\n",
        "      break\n",
        "    i += 1"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TransliterationCorpus\n",
            "Name\n",
            "SourceName\n",
            "TargetName\n",
            "Name\n",
            "SourceName\n",
            "TargetName\n",
            "Name\n",
            "SourceName\n",
            "TargetName\n",
            "Name\n",
            "SourceName\n",
            "TargetName\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "viobQC0t1xOa",
        "colab_type": "text"
      },
      "source": [
        "The above lists the inner elements.  \n",
        "The inner structure is :  \n",
        "TransliterationCorpus -> Name -> SourceName at same level as TargetName"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hSYQ8lzJ2SXV",
        "colab_type": "code",
        "outputId": "892c1077-6c57-46b3-f769-3f2b04949e28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        }
      },
      "source": [
        "# [(ele.tag, ele.attrib) for ele in root.iter(\"Name\")] : prints all attributes of Name tag\n",
        "# nextelem.tag or nextelem.attrib prints empty braces since this tag has no attributes\n",
        "\n",
        "i = 0\n",
        "for nextelem in root.iter(\"SourceName\"):\n",
        "  print(nextelem.text)\n",
        "  if i > 11:\n",
        "    break\n",
        "  i += 1"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Aadhi\n",
            "Aakash\n",
            "Aap\n",
            "Aayasha\n",
            "Aayee\n",
            "Abduh\n",
            "Aberhart\n",
            "Abey\n",
            "Abou\n",
            "Abri\n",
            "Academy\n",
            "Accommodation\n",
            "Acorn\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4E6o0BKE3kND",
        "colab_type": "code",
        "outputId": "cf6c4e7a-675e-4a93-da19-cfd5fb62425f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        }
      },
      "source": [
        "i = 0\n",
        "for nextelem in root.iter(\"TargetName\"):\n",
        "  print(nextelem.text)\n",
        "  if i > 11:\n",
        "    break\n",
        "  i += 1"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "आधी\n",
            "आकाश\n",
            "आप\n",
            "आयशा\n",
            "आई\n",
            "अब्दुस\n",
            "एबरहर्ट\n",
            "अबेय\n",
            "अबू\n",
            "एब्री\n",
            "अकेडमी\n",
            "एकेडमी\n",
            "एकोमडेशन\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6M4hyLxz3sqD",
        "colab_type": "text"
      },
      "source": [
        "Thus extracted English and corresponding Hindi from XML."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LPJlN53zvMR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TransliterationLoader(Dataset): # extends Dataset, adding features of our own to it\n",
        "  def __init__(self, filename):\n",
        "    self.eng_words, self.hindi_words = self.readXMLDataset(filename)\n",
        "    self.shuffle_indices = list(range(len(self.eng_words)))\n",
        "    random.shuffle(self.shuffle_indices)\n",
        "    self.shuffle_start_index = 0\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.eng_words)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.eng_words[idx], self.hindi_words[idx]\n",
        "\n",
        "  def readXMLDataset(self, filename):\n",
        "    transliterationCorpus = ET.parse(filename).getroot()\n",
        "    English_words = []\n",
        "    Hindi_words = []\n",
        "\n",
        "    for line in transliterationCorpus:\n",
        "      wordlist1 = cleanEnglishVocab(line[0].text)\n",
        "      wordlist2 = cleanHindiVocab(line[1].text)\n",
        "      # print(wordlist1, '-', wordlist2)\n",
        "\n",
        "      # Skip noisy data i.e. where number of \"words\" (not \"characters\" in each word) is not the same in English and Hindi\n",
        "      # e.g. English: Stirling Smith Museum And Art Gallery, Hindi: स (4 words in English corresponding to one in Hindi)\n",
        "      if len(wordlist1) != len(wordlist2):\n",
        "        print(\"Skipping: \", line[0].text, '-', line[1].text)\n",
        "        continue\n",
        "\n",
        "      # Since each tag may contain more than one word such as : ['STIRLING', 'SMITH', 'MUSEUM', 'AND', 'ART', 'GALLERY'], ['DEOGAN', 'ROAD']\n",
        "      # ['देवगन', 'रोड']\n",
        "      for word in wordlist1: \n",
        "        English_words.append(word)\n",
        "\n",
        "      for word in wordlist2: \n",
        "        Hindi_words.append(word)\n",
        "\n",
        "    return English_words, Hindi_words\n",
        "\n",
        "  def get_random_sample(self):\n",
        "    return self.__getitem__(np.random.randint(self.__len__()))\n",
        "\n",
        "  # generalise for Hindi and English (array is the parameter that decides if Hindi or English)\n",
        "  def get_batch_from_array(self, batch_size, array):\n",
        "    end = self.shuffle_start_index + batch_size\n",
        "    batch = []\n",
        "    if end >= self.__len__():\n",
        "      batch = [array[i] for i in self.shuffle_indices[0:end%self.__len__()]] # mod if batch size (\"end\") > the length of eng_words in the corpus we got from the XML file\n",
        "      end = self.__len__()\n",
        "    return batch + [array[i] for i in self.shuffle_indices[self.shuffle_start_index:end]]\n",
        "\n",
        "  def get_batch(self, batch_size, postprocess = True):\n",
        "    eng_batch = self.get_batch_from_array(batch_size, self.eng_words)\n",
        "    hindi_batch = self.get_batch_from_array(batch_size, self.hindi_words)\n",
        "    self.shuffle_start_index += batch_size + 1\n",
        "  \n",
        "    # Reshuffle if 1 epoch is complete\n",
        "    if self.shuffle_start_index >= self.__len__():\n",
        "      random.shuffle(self.shuffle_indices)\n",
        "      self.shuffle_start_index = 0\n",
        "\n",
        "    return eng_batch, hindi_batch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zI6zsl05UQO8",
        "colab_type": "code",
        "outputId": "9eda2be4-0017-4970-cec2-e2b233b782eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 741
        }
      },
      "source": [
        "train_data = TransliterationLoader('NEWS2012-Training-EnHi-13937.xml')\n",
        "test_data = TransliterationLoader('NEWS2012-Ref-EnHi-1000.xml')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Skipping:  BARHARWA JUNCTION - बरहरवा\n",
            "Skipping:  STATE BNK TR - स्टेट बैंक ऑफ त्रावणकोर\n",
            "Skipping:  SOUTH ARLINGTON CHURCH OF CHRIST - साउथ अर्लिंग्टन\n",
            "Skipping:  KING EDWARD VII - किंग एडवर्ड\n",
            "Skipping:  DIBANG VALLEY - दिबंगवैली\n",
            "Skipping:  ORDER OF VASA - ऑडर ऑफ़ द वासा\n",
            "Skipping:  AZAMNAGAR ROAD - आज़मनगर\n",
            "Skipping:  CAPE TOWN - केपटाउन\n",
            "Skipping:  NEW ZEALAND - न्यूज़ीलैंड\n",
            "Skipping:  SEA OF THE HEBRIDES - सी ऑफ हरब्रिड्‍स\n",
            "Skipping:  RAMCOIND - राम्को इंड\n",
            "Skipping:  KELVINGROVE ART GALLERY AND MUSEUM - केल्व‍िनग्रोव आर्ट एण्ड म्युज़ियम\n",
            "Skipping:  AUSTRALIAN NATIONAL UNIVERSITY - ऑस्ट्रेलियननेशनल यूनिवर्सिटी\n",
            "Skipping:  JAHAN AARA - जहाँआरा\n",
            "Skipping:  NAVABHARAT FERRO ALLOYS - नव भारत फ़ैरो अलॉय\n",
            "Skipping:  RAMA LINGESHWARA - रामालिंगेश्वर\n",
            "Skipping:  FAKHRUN NISA - फखरुन्निसा\n",
            "Skipping:  REDIFF.COM INDIA LIMITED - रेडिफ़ डॉट कॉम इंडिया लिमिटेड\n",
            "Skipping:  OMKARNATH THAKUR - ओंकार नाथ ठाकुर\n",
            "Skipping:  OPENTV - ओपन टीवी\n",
            "Skipping:  ENVOY COMMUNICATIONS GROUP - एन्वॉय कम्युनिकेशंस\n",
            "Skipping:  WAR OF THE HOLY LEAGUE - वार ऑफ होली लीग\n",
            "Skipping:  VAPARAISO CHURCH OF CHRIST - व्हापरासिओ\n",
            "Skipping:  PARIS CHARLES DE GAULLE - पेरिस रॉसे चार्ल्स डे ग्यूले\n",
            "Skipping:  PARKWAY APOSTOLIC - पार्क वे अपोस्टोलिक\n",
            "Skipping:  MAUNA LOA - मौनालोआ\n",
            "Skipping:  MASS MUTUAL LIFE - मास म्युच्युअल लाइफ़ इंश्योरेंस\n",
            "Skipping:  STATS CHIPPAC - स्टेट्सचिपपैक\n",
            "Skipping:  NEWFOUNDLAND - न्यू फाउंडलैंड\n",
            "Skipping:  LONDONHEATHROW - लंदन हीथ्रो\n",
            "Skipping:  RETALIX - रेटालिक्स लि.\n",
            "Skipping:  SRISAILAM - श्री शैलम\n",
            "Skipping:  KARA-KUM - काराकुम\n",
            "Skipping:  WIND RIVER - विंडरिवर\n",
            "Skipping:  NETAJI SUBHASH CHANDRA BOSE - नेताजी सुभाषचंद्र बोस\n",
            "Skipping:  ROCKBROOK UNITED - रॉकब्रुक यूनाइटेड मेथोडिस्ट\n",
            "Skipping:  WALTER SCOTT - वॉल्टरस्कॉट\n",
            "Skipping:  COLOURPLUS FASHIONS - कलर प्लस फ़ैशन्स\n",
            "Skipping:  BAL KRISHNA - बालकृष्णा\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yG5dpuiPUglk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNIof8hFaWra",
        "colab_type": "text"
      },
      "source": [
        "# Data Visualisation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "656wQxzhaZ2C",
        "colab_type": "code",
        "outputId": "6aa57d1b-90b2-455f-f6b3-5e5730f8d191",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277
        }
      },
      "source": [
        "print(\"Train set size: \", len(train_data))\n",
        "print(\"Test set size: \", len(test_data))\n",
        "\n",
        "print(\"\\nSample data from train set:\")\n",
        "for i in range(10):\n",
        "  eng, hindi = train_data.get_random_sample()\n",
        "  print(eng + '-' + hindi)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train set size:  20543\n",
            "Test set size:  1000\n",
            "\n",
            "Sample data from train set:\n",
            "SHASHAANK-शशांक\n",
            "JUNCTION-जंक्शन\n",
            "ALIMAA-अलीमा\n",
            "RON-रॉन\n",
            "LAGU-लागू\n",
            "TELEFIMS-टेलीफ़िल्म्स\n",
            "BANAR-बनार\n",
            "VIDHAN-विधान\n",
            "BOB-बॉब\n",
            "RHODES-रोड्स\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IscIrKxUgB2u",
        "colab_type": "text"
      },
      "source": [
        "(Note all english converted to upper case)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MrVYdVTrauCF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJA7cymMhIe8",
        "colab_type": "text"
      },
      "source": [
        "# Encoding single words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QBYkNaM4hJ80",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# OHE for English words (input) - convert word to number (OHE main purpose to convert to number)\n",
        "def word_rep( word, letter2index, device = 'cpu'):\n",
        "  rep = torch.zeros(len(word)+1, 1, len(letter2index)).to(device)\n",
        "  # (number of characters in word, batch in sequence problems, OHE length of each character in vocabulary i.e. 27)\n",
        "  for letter_index, letter in enumerate(word):\n",
        "    pos = letter2index[letter]\n",
        "    rep[letter_index][0][pos] = 1\n",
        "  pad_pos = letter2index[pad_char]\n",
        "  # marking the last character in the word as PAD (remember we made word size = actual word size + 1)\n",
        "  rep[letter_index+1][0][pad_pos] = 1\n",
        "  return rep\n",
        "\n",
        "\n",
        "# Just label for Hindi words (output) sice just classification task\n",
        "def gt_rep(word, letter2index, device = 'cpu'): # ground truth\n",
        "  gt_rep = torch.zeros([len(word)+1, 1], dtype = torch.long).to(device)\n",
        "  for letter_index, letter in enumerate(word):\n",
        "    pos = letter2index[letter]\n",
        "    gt_rep[letter_index][0] =  pos\n",
        "  gt_rep[letter_index + 1][0] = letter2index[pad_char]\n",
        "  return gt_rep"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3MA0GlwjzUD",
        "colab_type": "text"
      },
      "source": [
        "Get one pair of (English word, corresponding Hindi word)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPHQTkvYjyOf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "eng, hindi = train_data.get_random_sample()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NLsaIG8ajiJs",
        "colab_type": "code",
        "outputId": "d317b0d2-9ce0-48ff-e591-3cbe530fc6c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 574
        }
      },
      "source": [
        "eng_rep = word_rep(eng, eng_alpha2index)\n",
        "print(eng, \"\\n\", eng_rep)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "AMALATAAS \n",
            " tensor([[[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skF7lKe8kDJ_",
        "colab_type": "text"
      },
      "source": [
        "8 characters including PAD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ci7RZZ40jtcJ",
        "colab_type": "code",
        "outputId": "603b95fa-8ef3-4390-b668-ae0aafd81c4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        }
      },
      "source": [
        "hindi_gt = gt_rep(hindi, hindi_alpha2index)\n",
        "print(hindi, \"\\n\", hindi_gt)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "अमलतास \n",
            " tensor([[ 6],\n",
            "        [47],\n",
            "        [51],\n",
            "        [37],\n",
            "        [63],\n",
            "        [57],\n",
            "        [ 0]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTkPDB8wkQcA",
        "colab_type": "text"
      },
      "source": [
        "6 characters including PAD (kha, i, da, aa, sha, PAD)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2vBNRtAkrIZ",
        "colab_type": "text"
      },
      "source": [
        "***NOTE: Number of characters in input need not correspond to number of characters in output. That's the reason we use encoder-decoder architecture***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vOHZYI0Ij9K3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fPa5L0Z4lT0c",
        "colab_type": "text"
      },
      "source": [
        "# Network Architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aoOOHrzPlTtC",
        "colab_type": "text"
      },
      "source": [
        "Note: 3 letters in input (KHI) correspond to two letters in output (kha, i)  \n",
        "Similarly 2 letters in input (SH) correspond to one in output (sha)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqHWzuDMlp-r",
        "colab_type": "text"
      },
      "source": [
        "In such tasks, where output size != input size, need to encode entire input using the encoder part.  \n",
        "And then process it further using the decoder."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4y6Yf-jo43L",
        "colab_type": "text"
      },
      "source": [
        "## Encoder-Decoder Using GRU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eleXMZTYle75",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_OUTPUT_CHARS = 30\n",
        "class Transliteration_EncoderDecoder(nn.Module):\n",
        "\n",
        "  def __init__(self, input_size, hidden_size, output_size, verbose = False):\n",
        "    # input size is the size of each input character of the word i.e. 27 (OHE)\n",
        "    # output size is the size of each output word character i.e. 129 Hindi characters (Label)\n",
        "    # hidden_size is the number of layers (hyperparameters) of both encoder and decoder RNN cell (needn't be the same, can take 2 parameters for encoder hidden size and decoder hidden size)\n",
        "\n",
        "    super(Transliteration_EncoderDecoder, self).__init__()\n",
        "\n",
        "    self.hidden_size = hidden_size\n",
        "    self.output_size = output_size\n",
        "\n",
        "    # size of input to encoder cell is input_size\n",
        "    self.encoder_rnn_cell = nn.GRU(input_size, hidden_size)\n",
        "    # size of input to decoder cell is output_size, since the output of the previous timestamp is itself fed to the decoder cell of the current timestamp\n",
        "    self.decoder_rnn_cell = nn.GRU(output_size, hidden_size)\n",
        "\n",
        "    # fully connected\n",
        "    self.h2o = nn.Linear(hidden_size, output_size)\n",
        "    # softmax since classification\n",
        "    self.softmax = nn.LogSoftmax(dim=2)\n",
        "\n",
        "    self.verbose = verbose\n",
        "\n",
        "  def forward(self, input, max_output_chars = MAX_OUTPUT_CHARS, device = 'cpu', ground_truth = None):\n",
        "\n",
        "    # encoder\n",
        "\n",
        "    \"\"\"\n",
        "    Very important:\n",
        "    Encoder is a single GRU/LSTM cell (here, GRU). Now feeding to it (encoding) can be done in 2 ways when sequence (string) problems.\n",
        "    i) Feeding one character (vector size : 27), one at a time. And call it as many times as there are alphabets in the word. Thus input to the GRU once is (1,1,27)\n",
        "    ii) Feeding all the characters at once. In that case, input to GRU is (number of alphabets in word, 1, 27). In one go, computes output of the 6 characters. Internally it happens step wise - i.e. first cell, then output to second cell and so on.\n",
        "    This is possible, since encoding is a simple computation.\n",
        "    The dimension of the \"out\" correspondingly changes.\n",
        "    \"\"\"\n",
        "    out, hidden = self.encoder_rnn_cell(input)\n",
        "\n",
        "    if self.verbose:\n",
        "      print('Encoder input', input.shape)\n",
        "      print('Encoder ouput', out.shape)\n",
        "      print('Encoder hidden', hidden.shape)\n",
        "\n",
        "    # decoder\n",
        "    decoder_state = hidden # the first time the decoder state is set to value of last hidden layer of the encoder\n",
        "    decoder_input = torch.zeros(1, 1, self.output_size).to(device)\n",
        "    outputs = []\n",
        "\n",
        "    if self.verbose:\n",
        "      print('Decoder state', decoder_state.shape)\n",
        "      print('Decoder input', decoder_input.shape)\n",
        "\n",
        "    # unlike encoder, not going to invoke decoder in a single call, but loop through\n",
        "    for i in range(max_output_chars):\n",
        "\n",
        "      # the next time the decoder state is the hidden layer's output. Note hidden layer has two outputs (diagram). One to right that goes to hidden layer of next cell (timestamp) and the other that is the output of the cell (current character)\n",
        "      out, decoder_state = self.decoder_rnn_cell(decoder_input, decoder_state) # (input i.e. the previous Hindi char, state i.e. the output of the final hidden layer)\n",
        "\n",
        "      if self.verbose:\n",
        "        print('Decoder intermediate output', out.shape)\n",
        "\n",
        "      # Calculate final output of cell (1,1,129) from hidden output (1,1,256). h2o is defined in the init. A fully connected layer that converts 129 to 256\n",
        "      out = self.h2o(decoder_state) # why?? decoder_state is the output to the right.\n",
        "      out = self.softmax(out) # softmax\n",
        "      outputs.append(out.view(1, -1)) # the prediction of the current Hindi character appended to whole output word\n",
        "\n",
        "\n",
        "      \"\"\"\n",
        "      From here, conversion of the output of this layer to a form suitable for next layer\n",
        "      Instead of passing the softmax directly (1,1,129), we create a one hot encoding of it [ also (1,1,129) since that's the decoder input size ], where the highest softmax value is set as the index. And the rest to zero\n",
        "      Why required? Since the initial few times, we'll be sending the ground truth directly (which is in OHE), instead of the predicted output of the previous decoder cell since that leads to better training during the first few epochs till it learns a bit. After that continue training with the actual predicted output of the previous cell.\n",
        "      Allows to switch between ground truth (initially, when previous cell gives wrong prediction, at least make learning of next cell proper, else learns wrong since previous one's was wrong) and output of previous cell\n",
        "      Concept called \"Teacher Forcing\" : Accelerates training\n",
        "      \"\"\"\n",
        "\n",
        "    \n",
        "      if self.verbose:\n",
        "        print('Decoder output', out.shape)\n",
        "        self.verbose = False\n",
        "      \n",
        "      # Finding the argmax, marking that as 1, and the rest as 0\n",
        "      max_idx = torch.argmax(out, 2, keepdim = True)\n",
        "      if not ground_truth is None:\n",
        "        max_idx = ground_truth[i].reshape(1, 1, 1)\n",
        "      one_hot = torch.FloatTensor(out.shape).to(device)\n",
        "      one_hot.zero_()\n",
        "      one_hot.scatter_(2, max_idx, 1)\n",
        "\n",
        "\n",
        "      \"\"\"\n",
        "      PyTorch locks (remembers) every single parameter to backpropagate through, during gradient descent. But sometimes we don't want it to backpropagate through certain chains. E.g. Here output of previous cell goes to input of next timestamp cell. We don't want backpropagation to happen here since becomes too complicated. Only want through hidden layers. Hence detach (don't make note)\n",
        "      Even in the diagram, you can see there's no connection between previous layer's ouptut and next layer's input. Only connection is between hidden states, the only place we want backpropagation to take place\n",
        "      \"\"\"\n",
        "\n",
        "      decoder_input = one_hot.detach()\n",
        "\n",
        "    return outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHxGJbrzsMMX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Chosen number of hidden layers of both encoder and decoder to be 256\n",
        "net = Transliteration_EncoderDecoder(len(eng_alpha2index), 256, len(hindi_alpha2index), verbose = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OftZusrFxO92",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def infer(network, word, max_chars):\n",
        "  network.eval()\n",
        "  # network = network.__init__(len(eng_alphabets)+1, 256, len(hindi_alphabets)+1)\n",
        "  word = word_rep(word, eng_alpha2index)\n",
        "  print(word)\n",
        "\n",
        "  output = network(word, max_chars)\n",
        "  return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGA6QOhesWxK",
        "colab_type": "code",
        "outputId": "84028894-26ce-4851-936e-fb7e103b1e32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 463
        }
      },
      "source": [
        "out = infer(net, \"INDIA\", 30)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n",
            "Encoder input torch.Size([6, 1, 27])\n",
            "Encoder ouput torch.Size([6, 1, 256])\n",
            "Encoder hidden torch.Size([1, 1, 256])\n",
            "Decoder state torch.Size([1, 1, 256])\n",
            "Decoder input torch.Size([1, 1, 129])\n",
            "Decoder intermediate output torch.Size([1, 1, 256])\n",
            "Decoder output torch.Size([1, 1, 129])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxQXbCPEiJDU",
        "colab_type": "text"
      },
      "source": [
        "Input to encoder is of size (6,1,27), since OHE of all alphabets of the word fed at once.  \n",
        "Output of encoder is finally of size (6,1,256) too (final output after processing each).  \n",
        "Size of hidden layer of a single encoder cell is of course (1, 1, 256) since actually a cell processes only one word at a time. (Though finally it's 6 words).  \n",
        "Note that hidden layer is overwritten each time, hence finally it contains the parameters of the last processing only.  \n",
        "  \n",
        "Note that input is of size 27, but output is of size 256. So where is all that extra info coming from? It's capturing info about its vicinity. Thus learning about what's before, after, etc.   \n",
        "\n",
        "Remember in encoder, the output is the output of the hidden layer directly. No additional output layer , like softmax etc.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lB1FPN6ij47w",
        "colab_type": "text"
      },
      "source": [
        "Input to decoder (decoder state), is the output of the final hidden layer of the encoder (the last processing).  \n",
        "Input is the output of the previous decoder's timestamp i.e. (1,1,129). 129 chars in Hindi.  \n",
        "Output of hidden (intermediate) layer of decoder is same as that of encoder (1,1,256).  \n",
        "Again, output is just (1,1,129) i.e. a Hindi char produced at each timestamp."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPZ78ZhxuQs2",
        "colab_type": "code",
        "outputId": "f0487fdf-cd0d-4797-a219-ea47e6dc1cff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 593
        }
      },
      "source": [
        "print(len(out))\n",
        "for i in range(len(out)):\n",
        "  print(torch.argmax(out[i]))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "30\n",
            "tensor(9, grad_fn=<NotImplemented>)\n",
            "tensor(48, grad_fn=<NotImplemented>)\n",
            "tensor(4, grad_fn=<NotImplemented>)\n",
            "tensor(101, grad_fn=<NotImplemented>)\n",
            "tensor(4, grad_fn=<NotImplemented>)\n",
            "tensor(4, grad_fn=<NotImplemented>)\n",
            "tensor(64, grad_fn=<NotImplemented>)\n",
            "tensor(4, grad_fn=<NotImplemented>)\n",
            "tensor(64, grad_fn=<NotImplemented>)\n",
            "tensor(4, grad_fn=<NotImplemented>)\n",
            "tensor(64, grad_fn=<NotImplemented>)\n",
            "tensor(4, grad_fn=<NotImplemented>)\n",
            "tensor(64, grad_fn=<NotImplemented>)\n",
            "tensor(4, grad_fn=<NotImplemented>)\n",
            "tensor(64, grad_fn=<NotImplemented>)\n",
            "tensor(4, grad_fn=<NotImplemented>)\n",
            "tensor(64, grad_fn=<NotImplemented>)\n",
            "tensor(4, grad_fn=<NotImplemented>)\n",
            "tensor(64, grad_fn=<NotImplemented>)\n",
            "tensor(4, grad_fn=<NotImplemented>)\n",
            "tensor(64, grad_fn=<NotImplemented>)\n",
            "tensor(4, grad_fn=<NotImplemented>)\n",
            "tensor(64, grad_fn=<NotImplemented>)\n",
            "tensor(4, grad_fn=<NotImplemented>)\n",
            "tensor(64, grad_fn=<NotImplemented>)\n",
            "tensor(4, grad_fn=<NotImplemented>)\n",
            "tensor(64, grad_fn=<NotImplemented>)\n",
            "tensor(4, grad_fn=<NotImplemented>)\n",
            "tensor(64, grad_fn=<NotImplemented>)\n",
            "tensor(4, grad_fn=<NotImplemented>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W9eVQhK4ulK4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "117a07b4-c9f7-464c-b6d5-b2a52a807fca"
      },
      "source": [
        "list(hindi_alpha2index.values())"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0,\n",
              " 1,\n",
              " 2,\n",
              " 3,\n",
              " 4,\n",
              " 5,\n",
              " 6,\n",
              " 7,\n",
              " 8,\n",
              " 9,\n",
              " 10,\n",
              " 11,\n",
              " 12,\n",
              " 13,\n",
              " 14,\n",
              " 15,\n",
              " 16,\n",
              " 17,\n",
              " 18,\n",
              " 19,\n",
              " 20,\n",
              " 21,\n",
              " 22,\n",
              " 23,\n",
              " 24,\n",
              " 25,\n",
              " 26,\n",
              " 27,\n",
              " 28,\n",
              " 29,\n",
              " 30,\n",
              " 31,\n",
              " 32,\n",
              " 33,\n",
              " 34,\n",
              " 35,\n",
              " 36,\n",
              " 37,\n",
              " 38,\n",
              " 39,\n",
              " 40,\n",
              " 41,\n",
              " 42,\n",
              " 43,\n",
              " 44,\n",
              " 45,\n",
              " 46,\n",
              " 47,\n",
              " 48,\n",
              " 49,\n",
              " 50,\n",
              " 51,\n",
              " 52,\n",
              " 53,\n",
              " 54,\n",
              " 55,\n",
              " 56,\n",
              " 57,\n",
              " 58,\n",
              " 59,\n",
              " 60,\n",
              " 61,\n",
              " 62,\n",
              " 63,\n",
              " 64,\n",
              " 65,\n",
              " 66,\n",
              " 67,\n",
              " 68,\n",
              " 69,\n",
              " 70,\n",
              " 71,\n",
              " 72,\n",
              " 73,\n",
              " 74,\n",
              " 75,\n",
              " 76,\n",
              " 77,\n",
              " 78,\n",
              " 79,\n",
              " 80,\n",
              " 81,\n",
              " 82,\n",
              " 83,\n",
              " 84,\n",
              " 85,\n",
              " 86,\n",
              " 87,\n",
              " 88,\n",
              " 89,\n",
              " 90,\n",
              " 91,\n",
              " 92,\n",
              " 93,\n",
              " 94,\n",
              " 95,\n",
              " 96,\n",
              " 97,\n",
              " 98,\n",
              " 99,\n",
              " 100,\n",
              " 101,\n",
              " 102,\n",
              " 103,\n",
              " 104,\n",
              " 105,\n",
              " 106,\n",
              " 107,\n",
              " 108,\n",
              " 109,\n",
              " 110,\n",
              " 111,\n",
              " 112,\n",
              " 113,\n",
              " 114,\n",
              " 115,\n",
              " 116,\n",
              " 117,\n",
              " 118,\n",
              " 119,\n",
              " 120,\n",
              " 121,\n",
              " 122,\n",
              " 123,\n",
              " 124,\n",
              " 125,\n",
              " 126,\n",
              " 127,\n",
              " 128]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWujdA1fuNhK",
        "colab_type": "code",
        "outputId": "f38790a3-5c39-4dbe-9fdd-e6436a6f1d1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 593
        }
      },
      "source": [
        "print(len(out))\n",
        "for i in range(len(out)):\n",
        "  print([list(hindi_alpha2index.values()).index(torch.argmax(out[i]))])"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "30\n",
            "[9]\n",
            "[48]\n",
            "[4]\n",
            "[101]\n",
            "[4]\n",
            "[4]\n",
            "[64]\n",
            "[4]\n",
            "[64]\n",
            "[4]\n",
            "[64]\n",
            "[4]\n",
            "[64]\n",
            "[4]\n",
            "[64]\n",
            "[4]\n",
            "[64]\n",
            "[4]\n",
            "[64]\n",
            "[4]\n",
            "[64]\n",
            "[4]\n",
            "[64]\n",
            "[4]\n",
            "[64]\n",
            "[4]\n",
            "[64]\n",
            "[4]\n",
            "[64]\n",
            "[4]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APHsTSJP0qnh",
        "colab_type": "code",
        "outputId": "00e13cc9-1eb5-47ca-aa7b-a7db9aa88ce2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 593
        }
      },
      "source": [
        "print(len(out))\n",
        "for i in range(len(out)):\n",
        "  # basically indexing a list . The first \"list\" is the list to be indexed. The second \"list\" contains the index inside []\n",
        "  print(list(hindi_alpha2index.keys())[list(hindi_alpha2index.values()).index(torch.argmax(out[i]))])"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "30\n",
            "ई\n",
            "य\n",
            "ः\n",
            "।\n",
            "ः\n",
            "ः\n",
            "ि\n",
            "ः\n",
            "ि\n",
            "ः\n",
            "ि\n",
            "ः\n",
            "ि\n",
            "ः\n",
            "ि\n",
            "ः\n",
            "ि\n",
            "ः\n",
            "ि\n",
            "ः\n",
            "ि\n",
            "ः\n",
            "ि\n",
            "ः\n",
            "ि\n",
            "ः\n",
            "ि\n",
            "ः\n",
            "ि\n",
            "ः\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZfQVzW9XshhB",
        "colab_type": "code",
        "outputId": "de3b4bd7-f686-415a-9c72-b98e3fc18f28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 593
        }
      },
      "source": [
        "print(len(out))\n",
        "for i in range(len(out)):\n",
        "  print(out[i].shape, list(hindi_alpha2index.keys())[list(hindi_alpha2index.values()).index(torch.argmax(out[i]))])"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "30\n",
            "torch.Size([1, 129]) ई\n",
            "torch.Size([1, 129]) य\n",
            "torch.Size([1, 129]) ः\n",
            "torch.Size([1, 129]) ।\n",
            "torch.Size([1, 129]) ः\n",
            "torch.Size([1, 129]) ः\n",
            "torch.Size([1, 129]) ि\n",
            "torch.Size([1, 129]) ः\n",
            "torch.Size([1, 129]) ि\n",
            "torch.Size([1, 129]) ः\n",
            "torch.Size([1, 129]) ि\n",
            "torch.Size([1, 129]) ः\n",
            "torch.Size([1, 129]) ि\n",
            "torch.Size([1, 129]) ः\n",
            "torch.Size([1, 129]) ि\n",
            "torch.Size([1, 129]) ः\n",
            "torch.Size([1, 129]) ि\n",
            "torch.Size([1, 129]) ः\n",
            "torch.Size([1, 129]) ि\n",
            "torch.Size([1, 129]) ः\n",
            "torch.Size([1, 129]) ि\n",
            "torch.Size([1, 129]) ः\n",
            "torch.Size([1, 129]) ि\n",
            "torch.Size([1, 129]) ः\n",
            "torch.Size([1, 129]) ि\n",
            "torch.Size([1, 129]) ः\n",
            "torch.Size([1, 129]) ि\n",
            "torch.Size([1, 129]) ः\n",
            "torch.Size([1, 129]) ि\n",
            "torch.Size([1, 129]) ः\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6OdgFCFKvACD",
        "colab_type": "text"
      },
      "source": [
        "Completely senseless."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9s6y4gIu_Fc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HqG2602RClv4",
        "colab_type": "text"
      },
      "source": [
        "# Encoder Decoder with Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iaQpJNx9CQsO",
        "colab_type": "text"
      },
      "source": [
        "Not interested in working with the entire hidden state while predicting the first hindi character. Attention will only focus on those hidden states required for that character."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WyKy3LhFCgLo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Transliteration_EncoderDecoder_Attention(nn.Module):\n",
        "\n",
        "  def __init__(self, input_size, hidden_size, output_size, verbose = False):\n",
        "    super(Transliteration_EncoderDecoder_Attention, self).__init__()\n",
        "\n",
        "    self.hidden_size = hidden_size\n",
        "    self.output_size = output_size\n",
        "\n",
        "    # Only difference below, decoder is 2*hidden since going to augment\n",
        "    self.encoder_rnn_cell = nn.GRU(input_size, hidden_size)\n",
        "    self.decoder_rnn_cell = nn.GRU(hidden_size*2, hidden_size)\n",
        "\n",
        "    self.h2o = nn.Linear(hidden_size, output_size)\n",
        "    self.softmax = nn.LogSoftmax(dim = 2)\n",
        "\n",
        "    self.U = nn.Linear(self.hidden_size, self.hidden_size) # U\n",
        "    self.W = nn.Linear(self.hidden_size, self.hidden_size) # W\n",
        "    self.attn = nn.Linear(self.hidden_size, 1) # V\n",
        "    self.out2hidden = nn.Linear(self.output_size, self.hidden_size) # Convert ouptut of decoder to hidden_size too\n",
        "\n",
        "    self.verbose = verbose\n",
        "\n",
        "  def forward(self, input, max_output_chars = MAX_OUTPUT_CHARS, device = 'cpu', ground_truth = None):\n",
        "\n",
        "    # encoder\n",
        "    \"\"\"\n",
        "    Again note difference between encoder_output and hidden:\n",
        "    Encoder_output is the one coming to the top of the hidden state i.e. the alpha (which is what is considered in the linear combination of the weighted sum)\n",
        "    While hidden is the one  coming to the right of the hidden state i.e. what goes to the next encoder state.\n",
        "    Thus output is recorded for each cell i.e. each character (since not redirected anywhere else) while hidden is recorded only once since it is redirected to the next cell each time and hence overwritten\n",
        "    \"\"\"\n",
        "    encoder_outputs, hidden = self.encoder_rnn_cell(input) # input : (#alphabets in word, 1 ,27)\n",
        "    encoder_outputs = encoder_outputs.view(-1, self.hidden_size)\n",
        "\n",
        "    if self.verbose:\n",
        "      print(\"Encoder hidden\", hidden.shape)\n",
        "      print(\"Encoder output\", encoder_outputs.shape)\n",
        "\n",
        "    # decoder\n",
        "    decoder_state = hidden\n",
        "    decoder_input = torch.zeros(1, 1, self.output_size).to(device)\n",
        "    \n",
        "    outputs = []\n",
        "\n",
        "    # U matrix: multiplying each encoder ouput (alpha). Thus contains the \"weight of each input char for each output char\".\n",
        "    # Also note, this matrix is not changed in further code.\n",
        "    U = self.U(encoder_outputs)\n",
        "\n",
        "    if self.verbose:\n",
        "      print(\"Decoder state\", decoder_state.shape)\n",
        "      print(\"Decoder intermediate input\", decoder_input.shape)\n",
        "      print(\"U * Encoder output\", U.shape)\n",
        "\n",
        "\n",
        "    # Decode one by one unlike encoder. Decoding no more as simple as passing through GRU.\n",
        "    # Find W, Find V\n",
        "    for i in range(max_output_chars):\n",
        "\n",
        "      # repeat so that shape matches (as many number of times as chars)\n",
        "      W = self.W(decoder_state.view(1, -1).repeat(encoder_outputs.shape[0],1))\n",
        "      # again linear layer (attn) to calculate Vattn.\n",
        "      V = self.attn(torch.tanh(U + W))\n",
        "      # Softmax for finding alpha\n",
        "      attn_weights = F.softmax(V.view(1, -1), dim=1)\n",
        "\n",
        "      if self.verbose:\n",
        "        print(\"W * Decoder state\", W.shape)\n",
        "        print(\"V\", V.shape)\n",
        "        print(\"Attn\", attn_weights.shape)\n",
        "\n",
        "      # once weights found, find weighted sum with encoder outputs (say only one of the alphas is 1, giving attention to only that)\n",
        "      attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
        "                               encoder_outputs.unsqueeze(0))\n",
        "      \n",
        "      # Thus, till here, prepared all the encoder input to the decoder cell (combining encoder output and decoder previous timestamp \"hidden state\" [not \"output\"!])\n",
        "\n",
        "\n",
        "      # Now prepare shape of output of decoder for the next timestamp decoder cell\n",
        "      embedding = self.out2hidden(decoder_input)\n",
        "      # Apply linear between the ( previous decoder hidden state + encoder ouputs ) and ( previous decoder output ) so that both the kinds of input are appropriately represented. \n",
        "      decoder_input = torch.cat((embedding[0], attn_applied[0]), 1).unsqueeze(0)\n",
        "\n",
        "      if self.verbose:\n",
        "        print(\"Attn LC\", attn_applied.shape)\n",
        "        print(\"Decoder input\", decoder_input.shape)\n",
        "\n",
        "      out, decoder_state = self.decoder_rnn_cell(decoder_input, decoder_state)\n",
        "\n",
        "      if self.verbose:\n",
        "        print(\"Decoder intermediate output\", out.shape)\n",
        "\n",
        "      out = self.h2o(decoder_state)\n",
        "      out = self.softmax(out)\n",
        "      outputs.append(out.view(1, -1))\n",
        "\n",
        "      if self.verbose:\n",
        "        print('Decoder output', out.shape)\n",
        "        self.verbose = False\n",
        "\n",
        "      # Note in all this, U remains same, since only decoder we have to compute each time while U coming from encoder side remains the same throughout\n",
        "\n",
        "      # Forced teaching\n",
        "      max_idx = torch.argmax(out, 2, keepdim = True)\n",
        "      # i.e. if ground truth passed, use that instead of the previous decoder's output\n",
        "      if not ground_truth is None:\n",
        "        max_idx = ground_truth[i].reshape(1,1,1)\n",
        "      one_hot = torch.zeros(out.shape, device = device)\n",
        "      one_hot.scatter_(2, max_idx, 1)\n",
        "\n",
        "      # replace the above converted decoder output, to next timestamp's decoder cell input\n",
        "      # detaching since we don't want backpropagation to flow through these paths\n",
        "      decoder_input = one_hot.detach()\n",
        "\n",
        "    return outputs   \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kuK5jorWOMvr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "net_attn = Transliteration_EncoderDecoder_Attention(len(eng_alpha2index), 256, len(hindi_alpha2index), verbose = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "onGqmLSMOdne",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 555
        },
        "outputId": "8c83b382-1b5d-4b8e-cce0-06ee5965601b"
      },
      "source": [
        "# Have to feed in the capitals, else throws error since we didn't encode lower case\n",
        "out = infer(net_attn, \"INDIA\", 30)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n",
            "Encoder hidden torch.Size([1, 1, 256])\n",
            "Encoder output torch.Size([6, 256])\n",
            "Decoder state torch.Size([1, 1, 256])\n",
            "Decoder intermediate input torch.Size([1, 1, 129])\n",
            "U * Encoder output torch.Size([6, 256])\n",
            "W * Decoder state torch.Size([6, 256])\n",
            "V torch.Size([6, 1])\n",
            "Attn torch.Size([1, 6])\n",
            "Attn LC torch.Size([1, 1, 256])\n",
            "Decoder input torch.Size([1, 1, 512])\n",
            "Decoder intermediate output torch.Size([1, 1, 256])\n",
            "Decoder output torch.Size([1, 1, 129])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHim9W_CPG1S",
        "colab_type": "text"
      },
      "source": [
        "Attn is the alpha values [1, 6"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ihsu9z8oOt6D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}