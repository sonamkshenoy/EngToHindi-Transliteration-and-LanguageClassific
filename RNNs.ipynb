{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNNs.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOofzIprinS9Y2ErBFKi9ot"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "JseoduTJFYcx",
        "colab_type": "code",
        "outputId": "9142069b-5330-4639-9125-7b9955104354",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "source": [
        "from io import open\n",
        "import os, string, random, time, math\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gX5B2pjlFzEf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xHysBclRF27-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "As0Y_XQPF9aV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from IPython.display import clear_output # clears output of cell, say to show multiple plots but show only most recent one"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MOJxIp9CGfPo",
        "colab_type": "text"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suHpYDj-Gl0s",
        "colab_type": "text"
      },
      "source": [
        "Task: To predict nationality a name (varying length) belongs to.  \n",
        "Therefore, sequence model since sequence of characters, have to encode each character and predict one class (usual softmax) \n",
        "\n",
        "  \n",
        "\n",
        "---\n",
        "\n",
        "  \n",
        "Sequence Tasks:  \n",
        "Sequence to Class (input is sequence, output is just one class) - what we are doing currently   \n",
        "Sequence to Sequence Type 1 (input is sequence, output is sequence where there is an output corresponding to each item in the input e.g. POS)  \n",
        "Sequence to Class (input is sequence, output is sequence, but there is not one output for each word, but be more or less e.g. transliteration - converting english word to hindi)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJYssHQlGf8H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iwi82u7QICNW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "languages = []\n",
        "data = []\n",
        "X = []\n",
        "y = []\n",
        "\n",
        "with open('name2lang.txt','r') as f:\n",
        "  for line in f:\n",
        "    line = line.split(',')\n",
        "    name = line[0].strip()\n",
        "    lang = line[1].strip()\n",
        "    if not lang in languages:\n",
        "      languages.append(lang)\n",
        "    X.append(name)\n",
        "    y.append(lang)\n",
        "    data.append((name, lang))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QIyM_zA-JQuL",
        "colab_type": "code",
        "outputId": "cbc2fa13-9f9d-4933-f4b7-b2053011d25d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "n_languages = len(languages)\n",
        "print(n_languages)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "18\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZnqf841JYn_",
        "colab_type": "text"
      },
      "source": [
        "There are 18 languages in this dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ba1oJ1DQJwD7",
        "colab_type": "code",
        "outputId": "c440c35c-2f7a-48db-dfa3-9d045a041567",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "print(languages)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Portuguese', 'Irish', 'Spanish', 'Vietnamese', 'Chinese', 'Greek', 'Czech', 'Dutch', 'Japanese', 'French', 'German', 'Scottish', 'English', 'Russian', 'Polish', 'Arabic', 'Korean', 'Italian']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjRUUc78JW6Q",
        "colab_type": "code",
        "outputId": "1289afff-5b95-4e7b-b6d1-f27a99f3fd93",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "print(data[0:10])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('Abreu', 'Portuguese'), ('Albuquerque', 'Portuguese'), ('Almeida', 'Portuguese'), ('Alves', 'Portuguese'), ('Araujo', 'Portuguese'), ('Araullo', 'Portuguese'), ('Barros', 'Portuguese'), ('Basurto', 'Portuguese'), ('Belo', 'Portuguese'), ('Cabral', 'Portuguese')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "moN5zPJ-JiSf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VIZZk9K3J30c",
        "colab_type": "text"
      },
      "source": [
        "# Train-test split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HL7IevTbJ4Nt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 61, stratify = y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_PdNkrFK1ac",
        "colab_type": "text"
      },
      "source": [
        "(Both X and y should have the same dimensions)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNLNGqZnKSne",
        "colab_type": "code",
        "outputId": "08d4022b-29c7-4ddb-8797-d02c6c199562",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "print(X_train[:20])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Kaufman', 'Chandler', 'Agar', 'Essa', 'Makhagonov', 'Prokudin', 'Porus', 'Lowry', 'Nezvigin', 'Cove', 'Peach', 'Newlands', 'Bajov', 'Chukhnovsky', 'Naser', 'Gaspirovich', 'Vykhodtsev', 'Chuvilkin', 'Maksimchuk', 'Dandy']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BaisbKIhLBXY",
        "colab_type": "code",
        "outputId": "74f71448-8182-402d-9fc6-b25402c2af67",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(len(X_train), len(X_test))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "16040 4010\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CXYqNsBxLFRn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBn7lQ90LMiJ",
        "colab_type": "text"
      },
      "source": [
        "# Encoding names and languages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jad2LUTILVh0",
        "colab_type": "text"
      },
      "source": [
        "Since output is a class, can use one hot encoding easily.  \n",
        "But input is a sequence, so have to encode each character."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRYjpnk5LOnl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# list of all characters possible in a name\n",
        "all_letters = string.ascii_letters + \" .,;\"\n",
        "n_letters = len(all_letters)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bx61BiXULyBU",
        "colab_type": "text"
      },
      "source": [
        "Encode the name"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3DE9m7GLrBu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let encoding be one hot encoding for each character, where the hot bit is the index of the char in the above string of all letters\n",
        "def name_rep(name):\n",
        "  rep = torch.zeros(len(name), 1, n_letters)\n",
        "  for index, letter in enumerate(name):\n",
        "    pos = all_letters.find(letter)\n",
        "    rep[index][0][pos] = 1\n",
        "  return rep"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XbUw7ngUMt_Z",
        "colab_type": "text"
      },
      "source": [
        "Encode the language"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IXRpPvQfMV1E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Don't require one hot encoding, just label sufficient\n",
        "def lang_rep(lang):\n",
        "  return torch.tensor([languages.index(lang)], dtype = torch.long)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOnKrgj3M_cW",
        "colab_type": "code",
        "outputId": "286c538c-236e-49fa-cbb0-b96b5cc9bb4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 635
        }
      },
      "source": [
        "name_rep('Almeida')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "          0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
              "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "          0., 0., 0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
              "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "          0., 0., 0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
              "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "          0., 0., 0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "          0., 0., 0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "          0., 0., 0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "          0., 0., 0., 0., 0.]],\n",
              "\n",
              "        [[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "          0., 0., 0., 0., 0.]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vaYe84s3NaHG",
        "colab_type": "text"
      },
      "source": [
        "7 letters in name, so 7 vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUMWrn8GNEtq",
        "colab_type": "code",
        "outputId": "e39e7c06-8048-4087-a9a4-74261ca14363",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "lang_rep('Dutch')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([7])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RIVBLiX0Ni2Q",
        "colab_type": "text"
      },
      "source": [
        "Label of Dutch in languages array"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLnwCHSSNRQ5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCqYCaSdNrkg",
        "colab_type": "text"
      },
      "source": [
        "# Basic Visualisation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKEVu3G4Nzo9",
        "colab_type": "text"
      },
      "source": [
        "Distribution of languages in our dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVLJnDUrNs9b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "count = {}\n",
        "\n",
        "for l in languages:\n",
        "  count[l] = 0\n",
        "for d in data:\n",
        "  count[d[1]] += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VRJA48r3N_oo",
        "colab_type": "code",
        "outputId": "5ffaf256-1365-4cf4-ba79-1e0e829c871d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "print(count)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'Portuguese': 74, 'Irish': 232, 'Spanish': 298, 'Vietnamese': 73, 'Chinese': 268, 'Greek': 203, 'Czech': 519, 'Dutch': 297, 'Japanese': 991, 'French': 277, 'German': 724, 'Scottish': 100, 'English': 3668, 'Russian': 9384, 'Polish': 139, 'Arabic': 2000, 'Korean': 94, 'Italian': 709}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NkSI0f0COErW",
        "colab_type": "text"
      },
      "source": [
        "Highly non-uniform"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-uNbA3HOAfC",
        "colab_type": "code",
        "outputId": "f9393c19-6b4e-42db-feae-3cb3f8eb1e46",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 314
        }
      },
      "source": [
        "plt_ = sns.barplot(list(count.keys()), list(count.values()))\n",
        "plt_.set_xticklabels(plt_.get_xticklabels(), rotation = 90)\n",
        "plt.show()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDtdhBFxOxMx",
        "colab_type": "text"
      },
      "source": [
        "Model might learn Russian names too well and predict them well since lots of data, but less training on the rest.  \n",
        "Hence, have to see not the overall accuracy, but accuracy for each language."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEvPNdeFPK6Q",
        "colab_type": "text"
      },
      "source": [
        "Therefore, baseline is not to pick names from each language equally, but pick according to the distribution, more for Russian."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4wZ2VSPPZwb",
        "colab_type": "text"
      },
      "source": [
        "Thus, baseline testing is easy, but training isn't"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58IbURhoOa1z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IF2otzAwPVfh",
        "colab_type": "text"
      },
      "source": [
        "# Basic network and testing inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJDvL-wkPYoy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RNN_net(nn.Module):\n",
        "  # just setting up layers, not computing anything\n",
        "  # input_size : size of encoding of one character in the input (i.e. number of letters, here)\n",
        "  # hidden_size : the hidden layer though looks like one (for a single unit of the RNN) can be any number of layers\n",
        "  # output_size : the number of labels (number of languages, here)\n",
        "  def __init__(self, input_size, hidden_size, output_size):\n",
        "    super(RNN_net, self).__init__() # parent's (NN's) init\n",
        "    self.hidden_size = hidden_size\n",
        "\n",
        "    # both below are Linear (fully connected)\n",
        "    # just mention input size and output size. Weights, biases etc taken care of by library (encapsulated)\n",
        "\n",
        "    # i2h is the intersection of the previous character's output and current input character, hence concatenation of hidden size (previous input size - coming out of hidden layer) and input size (current input size). Output should of course be hidden size again\n",
        "    self.i2h = nn.Linear(input_size + hidden_size, hidden_size) # \"concatenation (i.e. both preserved)\", not addition (corresponding elements added), of input and hidden of previous\n",
        "    \n",
        "    # i2o is the intersection of the h and the output o. (the final output layer part). Input is the computed h (hidden_size) and the current input (input_size)\n",
        "    self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
        "\n",
        "    # Note only 1 layer used for both above transformations, but can have more\n",
        "    \n",
        "    # Final output\n",
        "    self.softmax = nn.LogSoftmax(dim = 1)\n",
        "\n",
        "\n",
        "  # where layers are put to use\n",
        "  def forward(self, input_, hidden):\n",
        "    combined = torch.cat((input_, hidden), 1) # concatenate hidden and input in right direction (1, here. 1 means column always in a 2D vector, and usually \"along\". So concatenate - NOT ADDS - along row. Next to each other)\n",
        "    hidden = self.i2h(combined) # comput hidden value\n",
        "    output = self.i2o(combined) # compute output\n",
        "    output = self.softmax(output) # compute softmax\n",
        "    return output, hidden # return both output and hidden\n",
        "\n",
        "  # the first time (beginning of sequence) the hidden vector is not present, so call this function the first time\n",
        "  def init_hidden(self):\n",
        "    return torch.zeros(1, self.hidden_size) # Randomly initialised to 1, can use something else too"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67vAe0Kblyty",
        "colab_type": "text"
      },
      "source": [
        "Now, instantiate model (hyperparameters)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bkXTHyqYlyWy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# number of hidden layers\n",
        "n_hidden = 128 # compromise between overfitting and having less data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K57M2vKNTrrx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "net = RNN_net(n_letters, n_hidden, n_languages)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vm_XnQfvnFQE",
        "colab_type": "text"
      },
      "source": [
        "Now, unlike CNNs, infer before testing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-Xp-lKKmiRI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def infer(net, name):\n",
        "  net.eval() # put network in eval mode during inference (not train mode)\n",
        "  name_ohe = name_rep(name) # one hot encoding\n",
        "  hidden = net.init_hidden() # initialise hidden vector for first instance of RNN, using encapsulated function\n",
        "\n",
        "  # name_ohe size = (n_letters, 1, sizeofword)\n",
        "  # here, iterating through each character. For each character in OHE, invoking forward (\"net\")\n",
        "  for i in range(name_ohe.size()[0]):\n",
        "\n",
        "    # name_ohe[i] = a vector of first character of OHE of all letters in the word (since RNN always expects a vector - 1 dimension)\n",
        "    # note the instantiated hidden layer is used only once. Thereafter, the new computed hidden used each time\n",
        "    # \"output\" for each iteration is the output of FFNN of one RNN unit, for that iteration (thus, overwritten each time). Hence at end of word, \"output\" contains the output of the final unit of the RNN (answer)\n",
        "    \n",
        "    output, hidden = net(name_ohe[i], hidden)\n",
        "\n",
        "  return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3K4ORDtlqp8_",
        "colab_type": "text"
      },
      "source": [
        "While inferring, an optimisation could be done, by removing softmax layer in the \"forward\" function of the model, since argmax will remain same before and after an argmax. It's only a computationally heavy operation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDUp1n1NpwRy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "outputId": "ec5653b8-c118-47f3-a626-d2dfb17e6666"
      },
      "source": [
        "output = infer(net, X_train[0])\n",
        "\n",
        "print(X_train[0])\n",
        "\n",
        "# Softmax returned so argmax\n",
        "index = torch.argmax(output)\n",
        "print(output)\n",
        "print(index)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Kaufman\n",
            "tensor([[-3.0253, -2.9496, -2.7226, -2.7676, -2.7876, -2.8880, -2.8898, -2.9612,\n",
            "         -2.9226, -2.9314, -2.9943, -2.8570, -2.9803, -2.8965, -2.8943, -2.9274,\n",
            "         -2.7563, -2.9384]], grad_fn=<LogSoftmaxBackward>)\n",
            "tensor(2, grad_fn=<NotImplemented>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjYbVE7PrAVX",
        "colab_type": "text"
      },
      "source": [
        "Says Kaufman belongs to nationality of index 3.  \n",
        "Note: without training, outputs if all classes are roughly same - doesn't know how to differentiate yet."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rPiNVfDJqGmW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P539BqXQrgPH",
        "colab_type": "text"
      },
      "source": [
        "# Evaluate model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iLd5PlQhrhD8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# just to generate random inputs from the dataset since in the data, languages are present in an ordered manner\n",
        "def dataloader(npoints, X_, y_):\n",
        "  to_ret = []\n",
        "  for i in range(npoints):\n",
        "    index_ = np.random.randint(len(X_))\n",
        "    name, lang = X_[index_], y_[index_]\n",
        "    to_ret.append((name, lang, name_rep(name), lang_rep(lang))) # append() takes exactly one argument so append as tuple\n",
        "  return to_ret"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6zWhSohuEAe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ccd73907-de65-4431-aa26-ed025014312e"
      },
      "source": [
        "dataloader(2, X_train, y_train)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Kouri',\n",
              "  'Arabic',\n",
              "  tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "            0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "            0., 0., 0., 0., 0.]],\n",
              "  \n",
              "          [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
              "            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "            0., 0., 0., 0., 0.]],\n",
              "  \n",
              "          [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "            0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "            0., 0., 0., 0., 0.]],\n",
              "  \n",
              "          [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "            1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "            0., 0., 0., 0., 0.]],\n",
              "  \n",
              "          [[0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "            0., 0., 0., 0., 0.]]]),\n",
              "  tensor([15])),\n",
              " ('Church',\n",
              "  'English',\n",
              "  tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
              "            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "            0., 0., 0., 0., 0.]],\n",
              "  \n",
              "          [[0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "            0., 0., 0., 0., 0.]],\n",
              "  \n",
              "          [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "            0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "            0., 0., 0., 0., 0.]],\n",
              "  \n",
              "          [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "            1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "            0., 0., 0., 0., 0.]],\n",
              "  \n",
              "          [[0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "            0., 0., 0., 0., 0.]],\n",
              "  \n",
              "          [[0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "            0., 0., 0., 0., 0.]]]),\n",
              "  tensor([12]))]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3QHufzt9uOAl",
        "colab_type": "text"
      },
      "source": [
        "Output: Name, lang, name_enc, lang_enc"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BFPNrTg2sa0X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Generally classification eval done using top-k accuracy (ground truth in top-k predictions)\n",
        "def eval(net, n_points, k, X_, y_):\n",
        "\n",
        "  data_ = dataloader(n_points, X_, y)\n",
        "  correct = 0\n",
        "\n",
        "  for name, language, name_ohe, lang_rep in data_:\n",
        "\n",
        "    output = infer(net, name)\n",
        "    val, indices = output.topk(k) # topk - PyTorch function\n",
        "\n",
        "    if lang_rep in indices:\n",
        "      correct +=1\n",
        "\n",
        "  accuracy = correct/n_points\n",
        "  return accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_QNY5MKtSAW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3c87dc73-3397-4598-e65a-ca5389bfcf4e"
      },
      "source": [
        "eval(net, 100, 1, X_test, y_test)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.06"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJ41yysLtdMW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5907e014-910a-4925-e792-09c264918b18"
      },
      "source": [
        "eval(net, 100, 10, X_test, y_test)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.51"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qp7NVin7vCvJ",
        "colab_type": "text"
      },
      "source": [
        "Obviously greater for higher k since almost all languages predicted"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5-A_qHfu-Kr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_QJlZCjvdr_",
        "colab_type": "text"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQGh8fh5viuQ",
        "colab_type": "text"
      },
      "source": [
        "### Basic setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QE7phZSHvkrw",
        "colab_type": "text"
      },
      "source": [
        "Define loss function and backpropagation  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOSQ3zox1mYE",
        "colab_type": "text"
      },
      "source": [
        "Computing output several times and then backpropagating through them. Thus, going through same network many times. But all this book keeping (of all params) done for us by PyTorch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZO8qrxlbveUq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(net, opt, criterion, n_points): # criterion = loss function\n",
        "\n",
        "  # set all params to 0, in case they were retained from an earlier training\n",
        "  opt.zero_grad()\n",
        "  total_loss = 0\n",
        "\n",
        "  # choose random points\n",
        "  data_ = dataloader(n_points, X_train, y_train)\n",
        "\n",
        "  for name, language, name_ohe, lang_rep in data_:\n",
        "\n",
        "    hidden = net.init_hidden()\n",
        "\n",
        "    # pass each character\n",
        "    # creating computational graph (i.e. the params, output for each individual unit of the RNN - all being stored by PyTorch). While backpropagating, we backpropagate through all of these\n",
        "    for i in range(name_ohe.size()[0]):\n",
        "      output, hidden = net(name_ohe[i], hidden)\n",
        "\n",
        "    loss = criterion(output, lang_rep)\n",
        "\n",
        "    # computing gradients (a LOT done in this step behind scenes)\n",
        "    loss.backward(retain_graph = True)\n",
        "\n",
        "    total_loss += loss\n",
        "\n",
        "  # update params using the grad computed using loss.backward \n",
        "  # Just one step here\n",
        "  opt.step()\n",
        "\n",
        "  # divide since we aggregated loss for all points\n",
        "  return total_loss/n_points"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wiW7TjEXzFRy",
        "colab_type": "text"
      },
      "source": [
        "The above is for one batch of n_points. For all these points, computing one set of gradients and updating only once."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qifhCnx6w5BX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = nn.NLLLoss() # negative log likelihood loss because using softmax\n",
        "opt = optim.SGD(net.parameters(), lr = 0.01, momentum = 0.9)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2PkgkUnSxE7D",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "8f0f967c-0551-45db-a459-ab2c7ed4e0cf"
      },
      "source": [
        "%%time\n",
        "train(net, opt, criterion, 200)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 269 ms, sys: 11.6 ms, total: 281 ms\n",
            "Wall time: 299 ms\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(2.2162, grad_fn=<DivBackward0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sz8eD2Shz0So",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8e669152-35e5-4c19-bb87-8d428f60c0c3"
      },
      "source": [
        "# Top-1\n",
        "eval(net, 1000, 1, X_test, y_test)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6YDbTfrm0FDH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4guzIp-u2J2F",
        "colab_type": "text"
      },
      "source": [
        "### Full training setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gE39Kc3b2LiW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_setup(net, lr = 0.01, n_batches = 100, batch_size = 10, momentum = 0.9, display_freq = 5):\n",
        "\n",
        "  criterion = nn.NLLLoss()\n",
        "  opt = optim.SGD(net.parameters(), lr = lr, momentum = momentum)\n",
        "\n",
        "  loss_arr = np.zeros(n_batches + 1)\n",
        "\n",
        "  for i in range(n_batches):\n",
        "    loss_arr[i+1] = (loss_arr[i]*i + train(net, opt, criterion, batch_size))/(i+1)\n",
        "\n",
        "    if i%display_fre == display_freq-1:\n",
        "      clear_output(wait=True)\n",
        "\n",
        "      print(\"Iteration\",i,\"Top-1\", eval(net, len(X_test), 1, X_test, y_test), \"Top-2\", eval(net, len(X_test), 1, X_test, y_test))\n",
        "\n",
        "      plt.figure()\n",
        "      plt.plot(loss_arr[l:i], '=*')\n",
        "      plt.xlabel('Iteration')\n",
        "      plt.ylabel('Loss')\n",
        "      plt.show()\n",
        "      print(\"\\n\\n\")\n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TuBLOl9r3iiC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_hidden = 128\n",
        "net = RNN_net(n_letters, n_hidden, n_languages)\n",
        "train_setup(net, lr = 0.0005, n_batches = 100, batch_size = 256)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}